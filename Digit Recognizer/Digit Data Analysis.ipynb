{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.1.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**load csv raw dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train = pd.read_csv('./data/train.csv', encoding='utf-8')\n",
    "raw_test = pd.read_csv('./data/test.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. shape of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Dataset**  \n",
    "(42000, 785)\n",
    "    \n",
    "**Test Dataset**  \n",
    "(28000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42000, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28000, 784)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Columns of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Train Dataset and Test Dataset all has pixels of image\n",
    "* Train Dataset and label for each image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'pixel0', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5',\n",
       "       'pixel6', 'pixel7', 'pixel8',\n",
       "       ...\n",
       "       'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779',\n",
       "       'pixel780', 'pixel781', 'pixel782', 'pixel783'],\n",
       "      dtype='object', length=785)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['pixel0', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5', 'pixel6',\n",
       "       'pixel7', 'pixel8', 'pixel9',\n",
       "       ...\n",
       "       'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779',\n",
       "       'pixel780', 'pixel781', 'pixel782', 'pixel783'],\n",
       "      dtype='object', length=784)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data and pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_augmentation(x_train, y_train):\n",
    "    aug_x_train = list()\n",
    "    aug_y_train = list()\n",
    "    \n",
    "    for x, y in zip(x_train, y_train):\n",
    "        aug_x_train.append(x)\n",
    "        aug_y_train.append(y)\n",
    "    \n",
    "        cval = np.median(x)\n",
    "    \n",
    "        for _ in range(2):        \n",
    "            angle = np.random.randint(-15, 15, 1)\n",
    "            rot_x = ndimage.rotate(x, angle[0], reshape=False, cval=cval)\n",
    "\n",
    "            shift = np.random.randint(-2, 2, 2)\n",
    "            shift_x = ndimage.shift(rot_x, shift, cval=cval)\n",
    "            \n",
    "            aug_x_train.append(shift_x)\n",
    "            aug_y_train.append(y)\n",
    "\n",
    "    aug_x_train = np.array(aug_x_train)\n",
    "    aug_y_train = np.array(aug_y_train)\n",
    "            \n",
    "    return aug_x_train, aug_y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_digit():\n",
    "    train = pd.read_csv('./data/train.csv', encoding='utf-8')\n",
    "    test = pd.read_csv('./data/test.csv', encoding='utf-8')\n",
    "    \n",
    "    x_original = train.loc[:, 'pixel0':]\n",
    "    y_original = train.loc[:, 'label']\n",
    "    x_test = test.loc[:, 'pixel0':]\n",
    "    \n",
    "    x_original = np.array(x_original)\n",
    "    y_original = np.array(y_original)\n",
    "    x_test = np.array(x_test)\n",
    "\n",
    "    x_original = np.reshape(x_original, (-1, 28, 28))\n",
    "    x_test = np.reshape(x_test, (-1, 28, 28))\n",
    "\n",
    "    x_train, y_train = data_augmentation(x_original, y_original)\n",
    "    x_val, y_val = x_original, y_original\n",
    "        \n",
    "    x_train = x_train / 255.0\n",
    "    x_val = x_val / 255.0\n",
    "    x_test = x_test / 255.0\n",
    "    \n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_val = np.expand_dims(x_val, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "    \n",
    "    y_train = tf.one_hot(y_train, 10)\n",
    "    y_val = tf.one_hot(y_val, 10)\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val), (x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(126000, 28, 28, 1)\n",
      "(126000, 10)\n",
      "(42000, 28, 28, 1)\n",
      "(42000, 10)\n",
      "(28000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val), (x_test) = load_digit()\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***Print Sample Data Augmented***\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('***Print Sample Data Augmented***')\n",
    "\n",
    "for idx in range(1, 4):\n",
    "    plt.subplot(1, 5, idx)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(str(np.argmax(y_train[idx-1])))\n",
    "    plt.imshow(np.reshape(x_train[idx-1], (28, 28)))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNConv(keras.Model):\n",
    "    def __init__(self, filters, kernel_size, strides=1, padding='same'):\n",
    "        super(BNConv, self).__init__()\n",
    "        \n",
    "        self.conv = keras.layers.Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer='glorot_normal')\n",
    "        self.bn = keras.layers.BatchNormalization()\n",
    "        self.relu = keras.layers.Activation(keras.activations.relu)\n",
    "        \n",
    "    def call(self, x, trainig=False):\n",
    "        layer = self.conv(x)\n",
    "        layer = self.bn(layer)\n",
    "        layer = self.relu(layer)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNDense(keras.Model):\n",
    "    def __init__(self, units):\n",
    "        super(BNDense, self).__init__()\n",
    "        \n",
    "        self.dense = keras.layers.Dense(units=units, kernel_initializer='glorot_normal')\n",
    "        self.bn = keras.layers.BatchNormalization()\n",
    "        self.relu = keras.layers.Activation(keras.activations.relu)\n",
    "        \n",
    "    def call(self, x, trainig=False):\n",
    "        layer = self.dense(x)\n",
    "        layer = self.bn(layer)\n",
    "        layer = self.relu(layer)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DigitModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(DigitModel, self).__init__()\n",
    "        \n",
    "        self.conv1 = BNConv(filters=16, kernel_size=(3, 3), padding='same')\n",
    "        self.pool1 = keras.layers.MaxPool2D(padding='same')\n",
    "\n",
    "        self.conv2 = BNConv(filters=32, kernel_size=(3, 3), padding='same')\n",
    "        self.pool2 = keras.layers.MaxPool2D(padding='same')\n",
    "        \n",
    "        self.conv3 = BNConv(filters=64, kernel_size=(3, 3), padding='same')\n",
    "        self.pool3 = keras.layers.MaxPool2D(padding='same')\n",
    "        self.flat3 = keras.layers.Flatten()\n",
    "        \n",
    "        self.dense4 = BNDense(units=128)\n",
    "        self.dropout4 = keras.layers.Dropout(rate=0.5)\n",
    "        \n",
    "        self.dense5 = keras.layers.Dense(units=10)\n",
    "        \n",
    "    def call(self, x, training=False):\n",
    "        layer1 = self.conv1(x, training=training)\n",
    "        layer1 = self.pool1(layer1)\n",
    "        \n",
    "        layer2 = self.conv2(layer1, training=training)\n",
    "        layer2 = self.pool2(layer2)\n",
    "        \n",
    "        layer3 = self.conv3(layer2, training=training)\n",
    "        layer3 = self.pool3(layer3)\n",
    "        layer3 = self.flat3(layer3)\n",
    "        \n",
    "        layer4 = self.dense4(layer3, training=training)\n",
    "        layer4 = self.dropout4(layer4)\n",
    "        \n",
    "        y = self.dense5(layer4)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_models = 5\n",
    "models = list()\n",
    "for _ in range(num_models):\n",
    "    models.append(DigitModel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"digit_model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bn_conv (BNConv)             (None, 28, 28, 16)        224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 16)        0         \n",
      "_________________________________________________________________\n",
      "bn_conv_1 (BNConv)           (None, 14, 14, 32)        4768      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 7, 7, 32)          0         \n",
      "_________________________________________________________________\n",
      "bn_conv_2 (BNConv)           (None, 7, 7, 64)          18752     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "bn_dense (BNDense)           (None, 128)               131712    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 156,746\n",
      "Trainable params: 156,266\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "temp = keras.Input(shape=x_val.shape[1:])\n",
    "models[0](temp)\n",
    "models[0].summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### performance evaulation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, x, y, training=True):\n",
    "    logits = model(x, training=training)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(models, x, y, training=False):\n",
    "    prediction = np.zeros_like(y)\n",
    "    for model in models:\n",
    "        prediction += model(x, training=training)\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_fn(model, x, y, training=False):\n",
    "    prediction = predict(model, x, y, training=training)\n",
    "    correct = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "iterations = len(x_train) // batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### checkpoint and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(model, checkpoint_dir):\n",
    "    print('[*] find checkpoint at {}'.format(checkpoint_dir))\n",
    "    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)\n",
    "    \n",
    "    if ckpt:\n",
    "        latest_checkpoint_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "        checkpoint = tf.train.Checkpoint(dnn=model)\n",
    "        checkpoint.restore(os.path.join(checkpoint_dir, latest_checkpoint_name))\n",
    "        \n",
    "        counter = int(latest_checkpoint_name.split('-')[1])\n",
    "        print('[*] checkpoint {} restored'.format(latest_checkpoint_name))\n",
    "        return True, counter\n",
    "    else:\n",
    "        print('[*] checkpoint not found')\n",
    "        return False, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "checkpoint_dir_base = 'checkpoints'\n",
    "model_dir = 'cnn'\n",
    "log_dir = 'logs'\n",
    "\n",
    "checkpoint_dirs = list()\n",
    "checkpoint_prefixes = list()\n",
    "checkpoints = list()\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    ensemble_dir = 'ensemble_{}'.format(idx)\n",
    "    checkpoint_dir = os.path.join(current_dir, checkpoint_dir_base, model_dir, ensemble_dir)\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    checkpoint_dirs.append(checkpoint_dir)\n",
    "    \n",
    "    checkpoints.append(tf.train.Checkpoint(dnn=model))\n",
    "    \n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, model_dir)\n",
    "    checkpoint_prefixes.append(checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join(current_dir, log_dir, model_dir)\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_decay = keras.optimizers.schedules.ExponentialDecay(learning_rate, 10, 0.95, staircase=True)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data pipelining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "dataset_train = dataset_train.shuffle(150000)\n",
    "dataset_train = dataset_train.prefetch(150000)\n",
    "dataset_train = dataset_train.batch(batch_size)\n",
    "dataset_train = dataset_train.repeat()\n",
    "\n",
    "dataset_train_iterator = iter(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_val = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
    "dataset_val = dataset_val.prefetch(50000)\n",
    "dataset_val = dataset_val.batch(len(x_val))\n",
    "dataset_val = dataset_val.repeat()\n",
    "\n",
    "dataset_val_iterator = iter(dataset_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] find checkpoint at /Users/dhsong/Workspaces/Practices/Kaggle Data Analysis/Digit Recognizer/checkpoints/cnn/ensemble_0\n",
      "[*] checkpoint cnn-5905-121 restored\n",
      "[*] find checkpoint at /Users/dhsong/Workspaces/Practices/Kaggle Data Analysis/Digit Recognizer/checkpoints/cnn/ensemble_1\n",
      "[*] checkpoint cnn-5905-121 restored\n",
      "[*] find checkpoint at /Users/dhsong/Workspaces/Practices/Kaggle Data Analysis/Digit Recognizer/checkpoints/cnn/ensemble_2\n",
      "[*] checkpoint cnn-5905-121 restored\n",
      "[*] find checkpoint at /Users/dhsong/Workspaces/Practices/Kaggle Data Analysis/Digit Recognizer/checkpoints/cnn/ensemble_3\n",
      "[*] checkpoint cnn-5905-121 restored\n",
      "[*] find checkpoint at /Users/dhsong/Workspaces/Practices/Kaggle Data Analysis/Digit Recognizer/checkpoints/cnn/ensemble_4\n",
      "[*] checkpoint cnn-5905-121 restored\n"
     ]
    }
   ],
   "source": [
    "for model, checkpoint_dir in zip(models, checkpoint_dirs):\n",
    "    could_load, checkpoint_counter = load_checkpoint(model, checkpoint_dir)\n",
    "\n",
    "if could_load:\n",
    "    start_epoch = checkpoint_counter // iterations\n",
    "    start_iteration = checkpoint_counter % iterations\n",
    "    counter = checkpoint_counter\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    start_iteration = 0\n",
    "    counter = checkpoint_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer digit_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer digit_model_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer digit_model_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer digit_model_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer digit_model_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch    1 Iteration [   1/ 984] Batch loss= 2.9834 acc=12.50%\n",
      "Epoch    1 Iteration [  11/ 984] Batch loss= 1.1469 acc=48.44%\n",
      "Epoch    1 Iteration [  21/ 984] Batch loss= 0.7056 acc=16.41%\n",
      "Epoch    1 Iteration [  31/ 984] Batch loss= 0.4713 acc=12.50%\n",
      "Epoch    1 Iteration [  41/ 984] Batch loss= 0.3970 acc=7.03%\n",
      "Epoch    1 Iteration [  51/ 984] Batch loss= 0.3214 acc=17.19%\n",
      "Epoch    1 Iteration [  61/ 984] Batch loss= 0.3035 acc=10.94%\n",
      "Epoch    1 Iteration [  71/ 984] Batch loss= 0.4375 acc=10.16%\n",
      "Epoch    1 Iteration [  81/ 984] Batch loss= 0.4187 acc=11.72%\n",
      "Epoch    1 Iteration [  91/ 984] Batch loss= 0.3935 acc=6.25%\n",
      "Epoch    1 Iteration [ 101/ 984] Batch loss= 0.3044 acc=11.72%\n",
      "Epoch    1 Iteration [ 111/ 984] Batch loss= 0.3972 acc=7.81%\n",
      "Epoch    1 Iteration [ 121/ 984] Batch loss= 0.3233 acc=12.50%\n",
      "Epoch    1 Iteration [ 131/ 984] Batch loss= 0.3400 acc=7.81%\n",
      "Epoch    1 Iteration [ 141/ 984] Batch loss= 0.2651 acc=12.50%\n",
      "Epoch    1 Iteration [ 151/ 984] Batch loss= 0.2691 acc=14.06%\n",
      "Epoch    1 Iteration [ 161/ 984] Batch loss= 0.2901 acc=13.28%\n",
      "Epoch    1 Iteration [ 171/ 984] Batch loss= 0.2532 acc=10.94%\n",
      "Epoch    1 Iteration [ 181/ 984] Batch loss= 0.3906 acc=8.59%\n",
      "Epoch    1 Iteration [ 191/ 984] Batch loss= 0.3613 acc=14.06%\n",
      "Epoch    1 Iteration [ 201/ 984] Batch loss= 0.3461 acc=10.94%\n",
      "Epoch    1 Iteration [ 211/ 984] Batch loss= 0.3286 acc=14.06%\n",
      "Epoch    1 Iteration [ 221/ 984] Batch loss= 0.3445 acc=10.16%\n",
      "Epoch    1 Iteration [ 231/ 984] Batch loss= 0.2940 acc=12.50%\n",
      "Epoch    1 Iteration [ 241/ 984] Batch loss= 0.3436 acc=10.16%\n",
      "Epoch    1 Iteration [ 251/ 984] Batch loss= 0.3035 acc=10.16%\n",
      "Epoch    1 Iteration [ 261/ 984] Batch loss= 0.3134 acc=13.28%\n",
      "Epoch    1 Iteration [ 271/ 984] Batch loss= 0.2694 acc=10.94%\n",
      "Epoch    1 Iteration [ 281/ 984] Batch loss= 0.3266 acc=10.94%\n",
      "Epoch    1 Iteration [ 291/ 984] Batch loss= 0.2501 acc=7.81%\n",
      "Epoch    1 Iteration [ 301/ 984] Batch loss= 0.3768 acc=16.41%\n",
      "Epoch    1 Iteration [ 311/ 984] Batch loss= 0.2063 acc=14.84%\n",
      "Epoch    1 Iteration [ 321/ 984] Batch loss= 0.3098 acc=10.16%\n",
      "Epoch    1 Iteration [ 331/ 984] Batch loss= 0.2754 acc=13.28%\n",
      "Epoch    1 Iteration [ 341/ 984] Batch loss= 0.3627 acc=11.72%\n",
      "Epoch    1 Iteration [ 351/ 984] Batch loss= 0.3283 acc=8.59%\n",
      "Epoch    1 Iteration [ 361/ 984] Batch loss= 0.3131 acc=10.16%\n",
      "Epoch    1 Iteration [ 371/ 984] Batch loss= 0.3170 acc=15.62%\n",
      "Epoch    1 Iteration [ 381/ 984] Batch loss= 0.2866 acc=16.41%\n",
      "Epoch    1 Iteration [ 391/ 984] Batch loss= 0.3914 acc=14.06%\n",
      "Epoch    1 Iteration [ 401/ 984] Batch loss= 0.3478 acc=24.22%\n",
      "Epoch    1 Iteration [ 411/ 984] Batch loss= 0.3066 acc=20.31%\n",
      "Epoch    1 Iteration [ 421/ 984] Batch loss= 0.3567 acc=21.88%\n",
      "Epoch    1 Iteration [ 431/ 984] Batch loss= 0.2618 acc=25.78%\n",
      "Epoch    1 Iteration [ 441/ 984] Batch loss= 0.2837 acc=40.62%\n",
      "Epoch    1 Iteration [ 451/ 984] Batch loss= 0.3554 acc=44.53%\n",
      "Epoch    1 Iteration [ 461/ 984] Batch loss= 0.2877 acc=48.44%\n",
      "Epoch    1 Iteration [ 471/ 984] Batch loss= 0.3650 acc=54.69%\n",
      "Epoch    1 Iteration [ 481/ 984] Batch loss= 0.2799 acc=64.84%\n",
      "Epoch    1 Iteration [ 491/ 984] Batch loss= 0.2509 acc=69.53%\n",
      "Epoch    1 Iteration [ 501/ 984] Batch loss= 0.2861 acc=72.66%\n",
      "Epoch    1 Iteration [ 511/ 984] Batch loss= 0.2950 acc=74.22%\n",
      "Epoch    1 Iteration [ 521/ 984] Batch loss= 0.2626 acc=82.03%\n",
      "Epoch    1 Iteration [ 531/ 984] Batch loss= 0.2577 acc=83.59%\n",
      "Epoch    1 Iteration [ 541/ 984] Batch loss= 0.3052 acc=85.94%\n",
      "Epoch    1 Iteration [ 551/ 984] Batch loss= 0.2262 acc=88.28%\n",
      "Epoch    1 Iteration [ 561/ 984] Batch loss= 0.2805 acc=91.41%\n",
      "Epoch    1 Iteration [ 571/ 984] Batch loss= 0.2233 acc=95.31%\n",
      "Epoch    1 Iteration [ 581/ 984] Batch loss= 0.3300 acc=92.19%\n",
      "Epoch    1 Iteration [ 591/ 984] Batch loss= 0.3099 acc=89.84%\n",
      "Epoch    1 Iteration [ 601/ 984] Batch loss= 0.2730 acc=93.75%\n",
      "Epoch    1 Iteration [ 611/ 984] Batch loss= 0.2723 acc=92.19%\n",
      "Epoch    1 Iteration [ 621/ 984] Batch loss= 0.3005 acc=96.09%\n",
      "Epoch    1 Iteration [ 631/ 984] Batch loss= 0.3271 acc=95.31%\n",
      "Epoch    1 Iteration [ 641/ 984] Batch loss= 0.3312 acc=90.62%\n",
      "Epoch    1 Iteration [ 651/ 984] Batch loss= 0.2791 acc=93.75%\n",
      "Epoch    1 Iteration [ 661/ 984] Batch loss= 0.2935 acc=92.19%\n",
      "Epoch    1 Iteration [ 671/ 984] Batch loss= 0.2753 acc=94.53%\n",
      "Epoch    1 Iteration [ 681/ 984] Batch loss= 0.2884 acc=93.75%\n",
      "Epoch    1 Iteration [ 691/ 984] Batch loss= 0.2829 acc=94.53%\n",
      "Epoch    1 Iteration [ 701/ 984] Batch loss= 0.3488 acc=92.97%\n",
      "Epoch    1 Iteration [ 711/ 984] Batch loss= 0.3449 acc=96.09%\n",
      "Epoch    1 Iteration [ 721/ 984] Batch loss= 0.3061 acc=95.31%\n",
      "Epoch    1 Iteration [ 731/ 984] Batch loss= 0.2590 acc=96.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    1 Iteration [ 741/ 984] Batch loss= 0.2719 acc=96.88%\n",
      "Epoch    1 Iteration [ 751/ 984] Batch loss= 0.2612 acc=97.66%\n",
      "Epoch    1 Iteration [ 761/ 984] Batch loss= 0.2590 acc=97.66%\n",
      "Epoch    1 Iteration [ 771/ 984] Batch loss= 0.3525 acc=92.97%\n",
      "Epoch    1 Iteration [ 781/ 984] Batch loss= 0.3269 acc=96.09%\n",
      "Epoch    1 Iteration [ 791/ 984] Batch loss= 0.3476 acc=96.09%\n",
      "Epoch    1 Iteration [ 801/ 984] Batch loss= 0.2927 acc=96.09%\n",
      "Epoch    1 Iteration [ 811/ 984] Batch loss= 0.2714 acc=96.88%\n",
      "Epoch    1 Iteration [ 821/ 984] Batch loss= 0.2904 acc=93.75%\n",
      "Epoch    1 Iteration [ 831/ 984] Batch loss= 0.2378 acc=96.88%\n",
      "Epoch    1 Iteration [ 841/ 984] Batch loss= 0.2569 acc=98.44%\n",
      "Epoch    1 Iteration [ 851/ 984] Batch loss= 0.3194 acc=95.31%\n",
      "Epoch    1 Iteration [ 861/ 984] Batch loss= 0.2458 acc=95.31%\n",
      "Epoch    1 Iteration [ 871/ 984] Batch loss= 0.4290 acc=92.19%\n",
      "Epoch    1 Iteration [ 881/ 984] Batch loss= 0.3124 acc=95.31%\n",
      "Epoch    1 Iteration [ 891/ 984] Batch loss= 0.2127 acc=97.66%\n",
      "Epoch    1 Iteration [ 901/ 984] Batch loss= 0.2871 acc=96.09%\n",
      "Epoch    1 Iteration [ 911/ 984] Batch loss= 0.2289 acc=99.22%\n",
      "Epoch    1 Iteration [ 921/ 984] Batch loss= 0.2814 acc=96.88%\n",
      "Epoch    1 Iteration [ 931/ 984] Batch loss= 0.3184 acc=94.53%\n",
      "Epoch    1 Iteration [ 941/ 984] Batch loss= 0.2748 acc=96.88%\n",
      "Epoch    1 Iteration [ 951/ 984] Batch loss= 0.3126 acc=96.09%\n",
      "Epoch    1 Iteration [ 961/ 984] Batch loss= 0.2835 acc=96.09%\n",
      "Epoch    1 Iteration [ 971/ 984] Batch loss= 0.3182 acc=95.31%\n",
      "Epoch    1 Iteration [ 981/ 984] Batch loss= 0.2502 acc=97.66%\n",
      "*** 499.7289 Epoch    1 [T] loss= 0.3346 acc=55.24%\t[v] loss= 0.1446 acc=97.15%\t***\n",
      "\n",
      "Epoch    2 Iteration [   1/ 984] Batch loss= 0.3046 acc=95.83%\n",
      "Epoch    2 Iteration [  11/ 984] Batch loss= 0.3421 acc=95.31%\n",
      "Epoch    2 Iteration [  21/ 984] Batch loss= 0.3476 acc=94.53%\n",
      "Epoch    2 Iteration [  31/ 984] Batch loss= 0.3238 acc=95.31%\n",
      "Epoch    2 Iteration [  41/ 984] Batch loss= 0.2530 acc=99.22%\n",
      "Epoch    2 Iteration [  51/ 984] Batch loss= 0.2634 acc=97.66%\n",
      "Epoch    2 Iteration [  61/ 984] Batch loss= 0.2766 acc=93.75%\n",
      "Epoch    2 Iteration [  71/ 984] Batch loss= 0.3003 acc=96.09%\n",
      "Epoch    2 Iteration [  81/ 984] Batch loss= 0.3380 acc=95.31%\n",
      "Epoch    2 Iteration [  91/ 984] Batch loss= 0.3759 acc=92.97%\n",
      "Epoch    2 Iteration [ 101/ 984] Batch loss= 0.3199 acc=93.75%\n",
      "Epoch    2 Iteration [ 111/ 984] Batch loss= 0.3789 acc=92.19%\n",
      "Epoch    2 Iteration [ 121/ 984] Batch loss= 0.3145 acc=94.53%\n",
      "Epoch    2 Iteration [ 131/ 984] Batch loss= 0.3218 acc=94.53%\n",
      "Epoch    2 Iteration [ 141/ 984] Batch loss= 0.2316 acc=96.88%\n",
      "Epoch    2 Iteration [ 151/ 984] Batch loss= 0.3423 acc=94.53%\n",
      "Epoch    2 Iteration [ 161/ 984] Batch loss= 0.3142 acc=96.09%\n",
      "Epoch    2 Iteration [ 171/ 984] Batch loss= 0.3314 acc=94.53%\n",
      "Epoch    2 Iteration [ 181/ 984] Batch loss= 0.2819 acc=97.66%\n",
      "Epoch    2 Iteration [ 191/ 984] Batch loss= 0.2990 acc=94.53%\n",
      "Epoch    2 Iteration [ 201/ 984] Batch loss= 0.2627 acc=98.44%\n",
      "Epoch    2 Iteration [ 211/ 984] Batch loss= 0.3268 acc=94.53%\n",
      "Epoch    2 Iteration [ 221/ 984] Batch loss= 0.2916 acc=97.66%\n",
      "Epoch    2 Iteration [ 231/ 984] Batch loss= 0.3321 acc=95.31%\n",
      "Epoch    2 Iteration [ 241/ 984] Batch loss= 0.2776 acc=96.09%\n",
      "Epoch    2 Iteration [ 251/ 984] Batch loss= 0.2410 acc=99.22%\n",
      "Epoch    2 Iteration [ 261/ 984] Batch loss= 0.3380 acc=92.19%\n",
      "Epoch    2 Iteration [ 271/ 984] Batch loss= 0.3136 acc=95.31%\n",
      "Epoch    2 Iteration [ 281/ 984] Batch loss= 0.3185 acc=96.88%\n",
      "Epoch    2 Iteration [ 291/ 984] Batch loss= 0.3732 acc=92.97%\n",
      "Epoch    2 Iteration [ 301/ 984] Batch loss= 0.3283 acc=93.75%\n",
      "Epoch    2 Iteration [ 311/ 984] Batch loss= 0.2911 acc=96.88%\n",
      "Epoch    2 Iteration [ 321/ 984] Batch loss= 0.2744 acc=97.66%\n",
      "Epoch    2 Iteration [ 331/ 984] Batch loss= 0.2404 acc=97.66%\n",
      "Epoch    2 Iteration [ 341/ 984] Batch loss= 0.2887 acc=96.88%\n",
      "Epoch    2 Iteration [ 351/ 984] Batch loss= 0.2696 acc=97.66%\n",
      "Epoch    2 Iteration [ 361/ 984] Batch loss= 0.3770 acc=96.09%\n",
      "Epoch    2 Iteration [ 371/ 984] Batch loss= 0.2624 acc=96.88%\n",
      "Epoch    2 Iteration [ 381/ 984] Batch loss= 0.2765 acc=97.66%\n",
      "Epoch    2 Iteration [ 391/ 984] Batch loss= 0.2848 acc=96.88%\n",
      "Epoch    2 Iteration [ 401/ 984] Batch loss= 0.3457 acc=94.53%\n",
      "Epoch    2 Iteration [ 411/ 984] Batch loss= 0.2936 acc=96.09%\n",
      "Epoch    2 Iteration [ 421/ 984] Batch loss= 0.2866 acc=96.09%\n",
      "Epoch    2 Iteration [ 431/ 984] Batch loss= 0.3326 acc=95.31%\n",
      "Epoch    2 Iteration [ 441/ 984] Batch loss= 0.3633 acc=92.19%\n",
      "Epoch    2 Iteration [ 451/ 984] Batch loss= 0.3475 acc=95.31%\n",
      "Epoch    2 Iteration [ 461/ 984] Batch loss= 0.3247 acc=96.09%\n",
      "Epoch    2 Iteration [ 471/ 984] Batch loss= 0.2444 acc=98.44%\n",
      "Epoch    2 Iteration [ 481/ 984] Batch loss= 0.2333 acc=96.88%\n",
      "Epoch    2 Iteration [ 491/ 984] Batch loss= 0.3151 acc=95.31%\n",
      "Epoch    2 Iteration [ 501/ 984] Batch loss= 0.2620 acc=96.88%\n",
      "Epoch    2 Iteration [ 511/ 984] Batch loss= 0.2850 acc=95.31%\n",
      "Epoch    2 Iteration [ 521/ 984] Batch loss= 0.3062 acc=95.31%\n",
      "Epoch    2 Iteration [ 531/ 984] Batch loss= 0.2758 acc=96.09%\n",
      "Epoch    2 Iteration [ 541/ 984] Batch loss= 0.3185 acc=94.53%\n",
      "Epoch    2 Iteration [ 551/ 984] Batch loss= 0.2672 acc=96.09%\n",
      "Epoch    2 Iteration [ 561/ 984] Batch loss= 0.2546 acc=97.66%\n",
      "Epoch    2 Iteration [ 571/ 984] Batch loss= 0.3006 acc=94.53%\n",
      "Epoch    2 Iteration [ 581/ 984] Batch loss= 0.2882 acc=96.88%\n",
      "Epoch    2 Iteration [ 591/ 984] Batch loss= 0.2937 acc=96.88%\n",
      "Epoch    2 Iteration [ 601/ 984] Batch loss= 0.2345 acc=100.00%\n",
      "Epoch    2 Iteration [ 611/ 984] Batch loss= 0.3056 acc=93.75%\n",
      "Epoch    2 Iteration [ 621/ 984] Batch loss= 0.2917 acc=98.44%\n",
      "Epoch    2 Iteration [ 631/ 984] Batch loss= 0.2324 acc=97.66%\n",
      "Epoch    2 Iteration [ 641/ 984] Batch loss= 0.3097 acc=97.66%\n",
      "Epoch    2 Iteration [ 651/ 984] Batch loss= 0.3182 acc=94.53%\n",
      "Epoch    2 Iteration [ 661/ 984] Batch loss= 0.3070 acc=96.88%\n",
      "Epoch    2 Iteration [ 671/ 984] Batch loss= 0.2071 acc=98.44%\n",
      "Epoch    2 Iteration [ 681/ 984] Batch loss= 0.3098 acc=95.31%\n",
      "Epoch    2 Iteration [ 691/ 984] Batch loss= 0.2710 acc=96.09%\n",
      "Epoch    2 Iteration [ 701/ 984] Batch loss= 0.2996 acc=96.09%\n",
      "Epoch    2 Iteration [ 711/ 984] Batch loss= 0.2998 acc=96.09%\n",
      "Epoch    2 Iteration [ 721/ 984] Batch loss= 0.3306 acc=93.75%\n",
      "Epoch    2 Iteration [ 731/ 984] Batch loss= 0.3027 acc=96.09%\n",
      "Epoch    2 Iteration [ 741/ 984] Batch loss= 0.2784 acc=96.88%\n",
      "Epoch    2 Iteration [ 751/ 984] Batch loss= 0.3444 acc=92.19%\n",
      "Epoch    2 Iteration [ 761/ 984] Batch loss= 0.3033 acc=95.31%\n",
      "Epoch    2 Iteration [ 771/ 984] Batch loss= 0.2424 acc=96.09%\n",
      "Epoch    2 Iteration [ 781/ 984] Batch loss= 0.3388 acc=92.97%\n",
      "Epoch    2 Iteration [ 791/ 984] Batch loss= 0.2108 acc=100.00%\n",
      "Epoch    2 Iteration [ 801/ 984] Batch loss= 0.2937 acc=96.88%\n",
      "Epoch    2 Iteration [ 811/ 984] Batch loss= 0.3011 acc=96.88%\n",
      "Epoch    2 Iteration [ 821/ 984] Batch loss= 0.2887 acc=94.53%\n",
      "Epoch    2 Iteration [ 831/ 984] Batch loss= 0.3430 acc=94.53%\n",
      "Epoch    2 Iteration [ 841/ 984] Batch loss= 0.2781 acc=96.88%\n",
      "Epoch    2 Iteration [ 851/ 984] Batch loss= 0.2917 acc=94.53%\n",
      "Epoch    2 Iteration [ 861/ 984] Batch loss= 0.2629 acc=97.66%\n",
      "Epoch    2 Iteration [ 871/ 984] Batch loss= 0.3462 acc=95.31%\n",
      "Epoch    2 Iteration [ 881/ 984] Batch loss= 0.3276 acc=95.31%\n",
      "Epoch    2 Iteration [ 891/ 984] Batch loss= 0.2735 acc=94.53%\n",
      "Epoch    2 Iteration [ 901/ 984] Batch loss= 0.3279 acc=92.97%\n",
      "Epoch    2 Iteration [ 911/ 984] Batch loss= 0.3414 acc=95.31%\n",
      "Epoch    2 Iteration [ 921/ 984] Batch loss= 0.2649 acc=96.09%\n",
      "Epoch    2 Iteration [ 931/ 984] Batch loss= 0.2918 acc=96.09%\n",
      "Epoch    2 Iteration [ 941/ 984] Batch loss= 0.2921 acc=96.88%\n",
      "Epoch    2 Iteration [ 951/ 984] Batch loss= 0.2869 acc=96.09%\n",
      "Epoch    2 Iteration [ 961/ 984] Batch loss= 0.3344 acc=94.53%\n",
      "Epoch    2 Iteration [ 971/ 984] Batch loss= 0.2912 acc=96.09%\n",
      "Epoch    2 Iteration [ 981/ 984] Batch loss= 0.3013 acc=96.88%\n",
      "*** 490.7368 Epoch    2 [T] loss= 0.2999 acc=95.69%\t[v] loss= 0.1446 acc=97.14%\t***\n",
      "\n",
      "Epoch    3 Iteration [   1/ 984] Batch loss= 0.2736 acc=96.09%\n",
      "Epoch    3 Iteration [  11/ 984] Batch loss= 0.3181 acc=95.31%\n",
      "Epoch    3 Iteration [  21/ 984] Batch loss= 0.3080 acc=96.88%\n",
      "Epoch    3 Iteration [  31/ 984] Batch loss= 0.3264 acc=95.31%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    3 Iteration [  41/ 984] Batch loss= 0.2608 acc=97.66%\n",
      "Epoch    3 Iteration [  51/ 984] Batch loss= 0.2587 acc=97.66%\n",
      "Epoch    3 Iteration [  61/ 984] Batch loss= 0.3353 acc=95.31%\n",
      "Epoch    3 Iteration [  71/ 984] Batch loss= 0.3478 acc=92.97%\n",
      "Epoch    3 Iteration [  81/ 984] Batch loss= 0.2517 acc=97.66%\n",
      "Epoch    3 Iteration [  91/ 984] Batch loss= 0.2655 acc=94.53%\n",
      "Epoch    3 Iteration [ 101/ 984] Batch loss= 0.3292 acc=96.09%\n",
      "Epoch    3 Iteration [ 111/ 984] Batch loss= 0.3240 acc=96.88%\n",
      "Epoch    3 Iteration [ 121/ 984] Batch loss= 0.3211 acc=95.31%\n",
      "Epoch    3 Iteration [ 131/ 984] Batch loss= 0.3027 acc=96.09%\n",
      "Epoch    3 Iteration [ 141/ 984] Batch loss= 0.4478 acc=92.19%\n",
      "Epoch    3 Iteration [ 151/ 984] Batch loss= 0.2918 acc=97.66%\n",
      "Epoch    3 Iteration [ 161/ 984] Batch loss= 0.4023 acc=92.97%\n",
      "Epoch    3 Iteration [ 171/ 984] Batch loss= 0.3368 acc=93.75%\n",
      "Epoch    3 Iteration [ 181/ 984] Batch loss= 0.3301 acc=95.31%\n",
      "Epoch    3 Iteration [ 191/ 984] Batch loss= 0.2622 acc=94.53%\n",
      "Epoch    3 Iteration [ 201/ 984] Batch loss= 0.2101 acc=97.66%\n",
      "Epoch    3 Iteration [ 211/ 984] Batch loss= 0.3390 acc=95.31%\n",
      "Epoch    3 Iteration [ 221/ 984] Batch loss= 0.3716 acc=95.31%\n",
      "Epoch    3 Iteration [ 231/ 984] Batch loss= 0.3177 acc=92.97%\n",
      "Epoch    3 Iteration [ 241/ 984] Batch loss= 0.3218 acc=94.53%\n",
      "Epoch    3 Iteration [ 251/ 984] Batch loss= 0.2203 acc=99.22%\n",
      "Epoch    3 Iteration [ 261/ 984] Batch loss= 0.3115 acc=94.53%\n",
      "Epoch    3 Iteration [ 271/ 984] Batch loss= 0.2624 acc=96.09%\n",
      "Epoch    3 Iteration [ 281/ 984] Batch loss= 0.2731 acc=98.44%\n",
      "Epoch    3 Iteration [ 291/ 984] Batch loss= 0.2754 acc=96.88%\n",
      "Epoch    3 Iteration [ 301/ 984] Batch loss= 0.3505 acc=92.97%\n",
      "Epoch    3 Iteration [ 311/ 984] Batch loss= 0.2211 acc=100.00%\n",
      "Epoch    3 Iteration [ 321/ 984] Batch loss= 0.2547 acc=97.66%\n",
      "Epoch    3 Iteration [ 331/ 984] Batch loss= 0.3326 acc=93.75%\n",
      "Epoch    3 Iteration [ 341/ 984] Batch loss= 0.3405 acc=92.97%\n",
      "Epoch    3 Iteration [ 351/ 984] Batch loss= 0.2979 acc=96.88%\n",
      "Epoch    3 Iteration [ 361/ 984] Batch loss= 0.2404 acc=99.22%\n",
      "Epoch    3 Iteration [ 371/ 984] Batch loss= 0.3459 acc=95.31%\n",
      "Epoch    3 Iteration [ 381/ 984] Batch loss= 0.2804 acc=97.66%\n",
      "Epoch    3 Iteration [ 391/ 984] Batch loss= 0.2390 acc=97.66%\n",
      "Epoch    3 Iteration [ 401/ 984] Batch loss= 0.3328 acc=94.53%\n",
      "Epoch    3 Iteration [ 411/ 984] Batch loss= 0.2608 acc=97.66%\n",
      "Epoch    3 Iteration [ 421/ 984] Batch loss= 0.2689 acc=96.88%\n",
      "Epoch    3 Iteration [ 431/ 984] Batch loss= 0.2940 acc=94.53%\n",
      "Epoch    3 Iteration [ 441/ 984] Batch loss= 0.2894 acc=97.66%\n",
      "Epoch    3 Iteration [ 451/ 984] Batch loss= 0.2754 acc=97.66%\n",
      "Epoch    3 Iteration [ 461/ 984] Batch loss= 0.2789 acc=96.88%\n",
      "Epoch    3 Iteration [ 471/ 984] Batch loss= 0.2831 acc=95.31%\n",
      "Epoch    3 Iteration [ 481/ 984] Batch loss= 0.3010 acc=95.31%\n",
      "Epoch    3 Iteration [ 491/ 984] Batch loss= 0.2821 acc=98.44%\n",
      "Epoch    3 Iteration [ 501/ 984] Batch loss= 0.2994 acc=95.31%\n",
      "Epoch    3 Iteration [ 511/ 984] Batch loss= 0.3600 acc=93.75%\n",
      "Epoch    3 Iteration [ 521/ 984] Batch loss= 0.3074 acc=96.09%\n",
      "Epoch    3 Iteration [ 531/ 984] Batch loss= 0.2691 acc=95.31%\n",
      "Epoch    3 Iteration [ 541/ 984] Batch loss= 0.2335 acc=97.66%\n",
      "Epoch    3 Iteration [ 551/ 984] Batch loss= 0.2455 acc=98.44%\n",
      "Epoch    3 Iteration [ 561/ 984] Batch loss= 0.2636 acc=96.09%\n",
      "Epoch    3 Iteration [ 571/ 984] Batch loss= 0.2562 acc=97.66%\n",
      "Epoch    3 Iteration [ 581/ 984] Batch loss= 0.3077 acc=95.31%\n",
      "Epoch    3 Iteration [ 591/ 984] Batch loss= 0.2850 acc=95.31%\n",
      "Epoch    3 Iteration [ 601/ 984] Batch loss= 0.3681 acc=92.97%\n",
      "Epoch    3 Iteration [ 611/ 984] Batch loss= 0.2803 acc=97.66%\n",
      "Epoch    3 Iteration [ 621/ 984] Batch loss= 0.3135 acc=96.88%\n",
      "Epoch    3 Iteration [ 631/ 984] Batch loss= 0.3537 acc=92.19%\n",
      "Epoch    3 Iteration [ 641/ 984] Batch loss= 0.3401 acc=95.31%\n",
      "Epoch    3 Iteration [ 651/ 984] Batch loss= 0.2700 acc=96.88%\n",
      "Epoch    3 Iteration [ 661/ 984] Batch loss= 0.2325 acc=97.66%\n",
      "Epoch    3 Iteration [ 671/ 984] Batch loss= 0.3824 acc=93.75%\n",
      "Epoch    3 Iteration [ 681/ 984] Batch loss= 0.2772 acc=97.66%\n",
      "Epoch    3 Iteration [ 691/ 984] Batch loss= 0.2208 acc=98.44%\n",
      "Epoch    3 Iteration [ 701/ 984] Batch loss= 0.2837 acc=96.09%\n",
      "Epoch    3 Iteration [ 711/ 984] Batch loss= 0.2864 acc=96.09%\n",
      "Epoch    3 Iteration [ 721/ 984] Batch loss= 0.3499 acc=94.53%\n",
      "Epoch    3 Iteration [ 731/ 984] Batch loss= 0.2766 acc=96.09%\n",
      "Epoch    3 Iteration [ 741/ 984] Batch loss= 0.3126 acc=96.09%\n",
      "Epoch    3 Iteration [ 751/ 984] Batch loss= 0.2576 acc=96.88%\n",
      "Epoch    3 Iteration [ 761/ 984] Batch loss= 0.2505 acc=96.09%\n",
      "Epoch    3 Iteration [ 771/ 984] Batch loss= 0.3067 acc=93.75%\n",
      "Epoch    3 Iteration [ 781/ 984] Batch loss= 0.3085 acc=96.09%\n",
      "Epoch    3 Iteration [ 791/ 984] Batch loss= 0.2512 acc=96.88%\n",
      "Epoch    3 Iteration [ 801/ 984] Batch loss= 0.3257 acc=95.31%\n",
      "Epoch    3 Iteration [ 811/ 984] Batch loss= 0.2600 acc=97.66%\n",
      "Epoch    3 Iteration [ 821/ 984] Batch loss= 0.2577 acc=97.66%\n",
      "Epoch    3 Iteration [ 831/ 984] Batch loss= 0.3258 acc=92.97%\n",
      "Epoch    3 Iteration [ 841/ 984] Batch loss= 0.2894 acc=96.88%\n",
      "Epoch    3 Iteration [ 851/ 984] Batch loss= 0.2773 acc=96.09%\n",
      "Epoch    3 Iteration [ 861/ 984] Batch loss= 0.2646 acc=96.88%\n",
      "Epoch    3 Iteration [ 871/ 984] Batch loss= 0.2725 acc=97.66%\n",
      "Epoch    3 Iteration [ 881/ 984] Batch loss= 0.3205 acc=96.09%\n",
      "Epoch    3 Iteration [ 891/ 984] Batch loss= 0.2605 acc=96.88%\n",
      "Epoch    3 Iteration [ 901/ 984] Batch loss= 0.2691 acc=97.66%\n",
      "Epoch    3 Iteration [ 911/ 984] Batch loss= 0.3063 acc=92.97%\n",
      "Epoch    3 Iteration [ 921/ 984] Batch loss= 0.1924 acc=99.22%\n",
      "Epoch    3 Iteration [ 931/ 984] Batch loss= 0.2209 acc=98.44%\n",
      "Epoch    3 Iteration [ 941/ 984] Batch loss= 0.3439 acc=95.31%\n",
      "Epoch    3 Iteration [ 951/ 984] Batch loss= 0.2345 acc=97.66%\n",
      "Epoch    3 Iteration [ 961/ 984] Batch loss= 0.3429 acc=95.31%\n",
      "Epoch    3 Iteration [ 971/ 984] Batch loss= 0.2426 acc=98.44%\n",
      "Epoch    3 Iteration [ 981/ 984] Batch loss= 0.3351 acc=92.97%\n",
      "*** 498.8940 Epoch    3 [T] loss= 0.2983 acc=95.71%\t[v] loss= 0.1446 acc=97.13%\t***\n",
      "\n",
      "Epoch    4 Iteration [   1/ 984] Batch loss= 0.3926 acc=92.97%\n",
      "Epoch    4 Iteration [  11/ 984] Batch loss= 0.3170 acc=95.31%\n",
      "Epoch    4 Iteration [  21/ 984] Batch loss= 0.3377 acc=96.09%\n",
      "Epoch    4 Iteration [  31/ 984] Batch loss= 0.2907 acc=94.53%\n",
      "Epoch    4 Iteration [  41/ 984] Batch loss= 0.2962 acc=96.88%\n",
      "Epoch    4 Iteration [  51/ 984] Batch loss= 0.2641 acc=97.66%\n",
      "Epoch    4 Iteration [  61/ 984] Batch loss= 0.2565 acc=96.88%\n",
      "Epoch    4 Iteration [  71/ 984] Batch loss= 0.2921 acc=96.09%\n",
      "Epoch    4 Iteration [  81/ 984] Batch loss= 0.3402 acc=94.53%\n",
      "Epoch    4 Iteration [  91/ 984] Batch loss= 0.3817 acc=95.31%\n",
      "Epoch    4 Iteration [ 101/ 984] Batch loss= 0.2554 acc=96.88%\n",
      "Epoch    4 Iteration [ 111/ 984] Batch loss= 0.3482 acc=94.53%\n",
      "Epoch    4 Iteration [ 121/ 984] Batch loss= 0.2813 acc=96.88%\n",
      "Epoch    4 Iteration [ 131/ 984] Batch loss= 0.2497 acc=97.66%\n",
      "Epoch    4 Iteration [ 141/ 984] Batch loss= 0.2414 acc=97.66%\n",
      "Epoch    4 Iteration [ 151/ 984] Batch loss= 0.3071 acc=95.31%\n",
      "Epoch    4 Iteration [ 161/ 984] Batch loss= 0.2852 acc=96.09%\n",
      "Epoch    4 Iteration [ 171/ 984] Batch loss= 0.2507 acc=98.44%\n",
      "Epoch    4 Iteration [ 181/ 984] Batch loss= 0.2959 acc=95.31%\n",
      "Epoch    4 Iteration [ 191/ 984] Batch loss= 0.2918 acc=96.88%\n",
      "Epoch    4 Iteration [ 201/ 984] Batch loss= 0.3728 acc=93.75%\n",
      "Epoch    4 Iteration [ 211/ 984] Batch loss= 0.3596 acc=94.53%\n",
      "Epoch    4 Iteration [ 221/ 984] Batch loss= 0.2711 acc=97.66%\n",
      "Epoch    4 Iteration [ 231/ 984] Batch loss= 0.2753 acc=96.88%\n",
      "Epoch    4 Iteration [ 241/ 984] Batch loss= 0.3317 acc=95.31%\n",
      "Epoch    4 Iteration [ 251/ 984] Batch loss= 0.2500 acc=97.66%\n",
      "Epoch    4 Iteration [ 261/ 984] Batch loss= 0.2804 acc=97.66%\n",
      "Epoch    4 Iteration [ 271/ 984] Batch loss= 0.3058 acc=96.88%\n",
      "Epoch    4 Iteration [ 281/ 984] Batch loss= 0.3411 acc=92.19%\n",
      "Epoch    4 Iteration [ 291/ 984] Batch loss= 0.2421 acc=96.88%\n",
      "Epoch    4 Iteration [ 301/ 984] Batch loss= 0.2979 acc=95.31%\n",
      "Epoch    4 Iteration [ 311/ 984] Batch loss= 0.2734 acc=95.31%\n",
      "Epoch    4 Iteration [ 321/ 984] Batch loss= 0.2375 acc=97.66%\n",
      "Epoch    4 Iteration [ 331/ 984] Batch loss= 0.2540 acc=96.88%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    4 Iteration [ 341/ 984] Batch loss= 0.3173 acc=95.31%\n",
      "Epoch    4 Iteration [ 351/ 984] Batch loss= 0.3455 acc=92.97%\n",
      "Epoch    4 Iteration [ 361/ 984] Batch loss= 0.3191 acc=94.53%\n",
      "Epoch    4 Iteration [ 371/ 984] Batch loss= 0.2532 acc=97.66%\n",
      "Epoch    4 Iteration [ 381/ 984] Batch loss= 0.2954 acc=95.31%\n",
      "Epoch    4 Iteration [ 391/ 984] Batch loss= 0.2986 acc=94.53%\n",
      "Epoch    4 Iteration [ 401/ 984] Batch loss= 0.2502 acc=95.31%\n",
      "Epoch    4 Iteration [ 411/ 984] Batch loss= 0.2960 acc=94.53%\n",
      "Epoch    4 Iteration [ 421/ 984] Batch loss= 0.2544 acc=98.44%\n",
      "Epoch    4 Iteration [ 431/ 984] Batch loss= 0.2545 acc=96.09%\n",
      "Epoch    4 Iteration [ 441/ 984] Batch loss= 0.3048 acc=95.31%\n",
      "Epoch    4 Iteration [ 451/ 984] Batch loss= 0.3071 acc=95.31%\n",
      "Epoch    4 Iteration [ 461/ 984] Batch loss= 0.2571 acc=96.88%\n",
      "Epoch    4 Iteration [ 471/ 984] Batch loss= 0.3462 acc=96.88%\n",
      "Epoch    4 Iteration [ 481/ 984] Batch loss= 0.2839 acc=96.09%\n",
      "Epoch    4 Iteration [ 491/ 984] Batch loss= 0.3559 acc=94.53%\n",
      "Epoch    4 Iteration [ 501/ 984] Batch loss= 0.2713 acc=96.09%\n",
      "Epoch    4 Iteration [ 511/ 984] Batch loss= 0.3155 acc=94.53%\n",
      "Epoch    4 Iteration [ 521/ 984] Batch loss= 0.2648 acc=96.88%\n",
      "Epoch    4 Iteration [ 531/ 984] Batch loss= 0.3423 acc=94.53%\n",
      "Epoch    4 Iteration [ 541/ 984] Batch loss= 0.3495 acc=96.09%\n",
      "Epoch    4 Iteration [ 551/ 984] Batch loss= 0.2554 acc=96.88%\n",
      "Epoch    4 Iteration [ 561/ 984] Batch loss= 0.2164 acc=98.44%\n",
      "Epoch    4 Iteration [ 571/ 984] Batch loss= 0.2726 acc=95.31%\n",
      "Epoch    4 Iteration [ 581/ 984] Batch loss= 0.2922 acc=95.31%\n",
      "Epoch    4 Iteration [ 591/ 984] Batch loss= 0.2725 acc=96.88%\n",
      "Epoch    4 Iteration [ 601/ 984] Batch loss= 0.2711 acc=96.09%\n",
      "Epoch    4 Iteration [ 611/ 984] Batch loss= 0.2979 acc=96.88%\n",
      "Epoch    4 Iteration [ 621/ 984] Batch loss= 0.3646 acc=94.53%\n",
      "Epoch    4 Iteration [ 631/ 984] Batch loss= 0.3711 acc=92.97%\n",
      "Epoch    4 Iteration [ 641/ 984] Batch loss= 0.2808 acc=96.88%\n",
      "Epoch    4 Iteration [ 651/ 984] Batch loss= 0.2233 acc=98.44%\n",
      "Epoch    4 Iteration [ 661/ 984] Batch loss= 0.3116 acc=95.31%\n",
      "Epoch    4 Iteration [ 671/ 984] Batch loss= 0.2793 acc=96.09%\n",
      "Epoch    4 Iteration [ 681/ 984] Batch loss= 0.2669 acc=97.66%\n",
      "Epoch    4 Iteration [ 691/ 984] Batch loss= 0.2990 acc=95.31%\n",
      "Epoch    4 Iteration [ 701/ 984] Batch loss= 0.3212 acc=96.88%\n",
      "Epoch    4 Iteration [ 711/ 984] Batch loss= 0.2598 acc=96.88%\n",
      "Epoch    4 Iteration [ 721/ 984] Batch loss= 0.1905 acc=98.44%\n",
      "Epoch    4 Iteration [ 731/ 984] Batch loss= 0.3762 acc=95.31%\n",
      "Epoch    4 Iteration [ 741/ 984] Batch loss= 0.3515 acc=94.53%\n",
      "Epoch    4 Iteration [ 751/ 984] Batch loss= 0.3502 acc=92.19%\n",
      "Epoch    4 Iteration [ 761/ 984] Batch loss= 0.2965 acc=96.09%\n",
      "Epoch    4 Iteration [ 771/ 984] Batch loss= 0.2834 acc=96.88%\n",
      "Epoch    4 Iteration [ 781/ 984] Batch loss= 0.3358 acc=96.88%\n",
      "Epoch    4 Iteration [ 791/ 984] Batch loss= 0.3401 acc=93.75%\n",
      "Epoch    4 Iteration [ 801/ 984] Batch loss= 0.3544 acc=93.75%\n",
      "Epoch    4 Iteration [ 811/ 984] Batch loss= 0.3229 acc=95.31%\n",
      "Epoch    4 Iteration [ 821/ 984] Batch loss= 0.2285 acc=98.44%\n",
      "Epoch    4 Iteration [ 831/ 984] Batch loss= 0.2852 acc=96.88%\n",
      "Epoch    4 Iteration [ 841/ 984] Batch loss= 0.2877 acc=95.31%\n",
      "Epoch    4 Iteration [ 851/ 984] Batch loss= 0.3324 acc=95.31%\n",
      "Epoch    4 Iteration [ 861/ 984] Batch loss= 0.2999 acc=96.09%\n",
      "Epoch    4 Iteration [ 871/ 984] Batch loss= 0.2568 acc=97.66%\n",
      "Epoch    4 Iteration [ 881/ 984] Batch loss= 0.2722 acc=96.09%\n",
      "Epoch    4 Iteration [ 891/ 984] Batch loss= 0.2455 acc=98.44%\n",
      "Epoch    4 Iteration [ 901/ 984] Batch loss= 0.3336 acc=96.09%\n",
      "Epoch    4 Iteration [ 911/ 984] Batch loss= 0.3801 acc=93.75%\n",
      "Epoch    4 Iteration [ 921/ 984] Batch loss= 0.3395 acc=92.19%\n",
      "Epoch    4 Iteration [ 931/ 984] Batch loss= 0.3072 acc=96.09%\n",
      "Epoch    4 Iteration [ 941/ 984] Batch loss= 0.3429 acc=94.53%\n",
      "Epoch    4 Iteration [ 951/ 984] Batch loss= 0.3156 acc=95.31%\n",
      "Epoch    4 Iteration [ 961/ 984] Batch loss= 0.3241 acc=92.19%\n",
      "Epoch    4 Iteration [ 971/ 984] Batch loss= 0.3309 acc=92.97%\n",
      "Epoch    4 Iteration [ 981/ 984] Batch loss= 0.2475 acc=97.66%\n",
      "*** 490.0850 Epoch    4 [T] loss= 0.2990 acc=95.70%\t[v] loss= 0.1445 acc=97.15%\t***\n",
      "\n",
      "Epoch    5 Iteration [   1/ 984] Batch loss= 0.3161 acc=94.53%\n",
      "Epoch    5 Iteration [  11/ 984] Batch loss= 0.4153 acc=93.75%\n",
      "Epoch    5 Iteration [  21/ 984] Batch loss= 0.2224 acc=97.66%\n",
      "Epoch    5 Iteration [  31/ 984] Batch loss= 0.3619 acc=94.53%\n",
      "Epoch    5 Iteration [  41/ 984] Batch loss= 0.2859 acc=95.31%\n",
      "Epoch    5 Iteration [  51/ 984] Batch loss= 0.3071 acc=96.88%\n",
      "Epoch    5 Iteration [  61/ 984] Batch loss= 0.2796 acc=96.09%\n",
      "Epoch    5 Iteration [  71/ 984] Batch loss= 0.3112 acc=94.53%\n",
      "Epoch    5 Iteration [  81/ 984] Batch loss= 0.2652 acc=96.09%\n",
      "Epoch    5 Iteration [  91/ 984] Batch loss= 0.3806 acc=92.97%\n",
      "Epoch    5 Iteration [ 101/ 984] Batch loss= 0.2728 acc=96.88%\n",
      "Epoch    5 Iteration [ 111/ 984] Batch loss= 0.3495 acc=92.19%\n",
      "Epoch    5 Iteration [ 121/ 984] Batch loss= 0.2433 acc=96.88%\n",
      "Epoch    5 Iteration [ 131/ 984] Batch loss= 0.2783 acc=97.66%\n",
      "Epoch    5 Iteration [ 141/ 984] Batch loss= 0.2432 acc=96.88%\n",
      "Epoch    5 Iteration [ 151/ 984] Batch loss= 0.3562 acc=94.53%\n",
      "Epoch    5 Iteration [ 161/ 984] Batch loss= 0.2953 acc=96.09%\n",
      "Epoch    5 Iteration [ 171/ 984] Batch loss= 0.2343 acc=96.88%\n",
      "Epoch    5 Iteration [ 181/ 984] Batch loss= 0.2034 acc=98.44%\n",
      "Epoch    5 Iteration [ 191/ 984] Batch loss= 0.3174 acc=95.31%\n",
      "Epoch    5 Iteration [ 201/ 984] Batch loss= 0.3272 acc=94.53%\n",
      "Epoch    5 Iteration [ 211/ 984] Batch loss= 0.2863 acc=96.09%\n",
      "Epoch    5 Iteration [ 221/ 984] Batch loss= 0.3296 acc=96.09%\n",
      "Epoch    5 Iteration [ 231/ 984] Batch loss= 0.3733 acc=92.97%\n",
      "Epoch    5 Iteration [ 241/ 984] Batch loss= 0.3081 acc=96.09%\n",
      "Epoch    5 Iteration [ 251/ 984] Batch loss= 0.3356 acc=94.53%\n",
      "Epoch    5 Iteration [ 261/ 984] Batch loss= 0.3194 acc=96.88%\n",
      "Epoch    5 Iteration [ 271/ 984] Batch loss= 0.2852 acc=96.09%\n",
      "Epoch    5 Iteration [ 281/ 984] Batch loss= 0.2831 acc=96.88%\n",
      "Epoch    5 Iteration [ 291/ 984] Batch loss= 0.2997 acc=96.09%\n",
      "Epoch    5 Iteration [ 301/ 984] Batch loss= 0.2789 acc=97.66%\n",
      "Epoch    5 Iteration [ 311/ 984] Batch loss= 0.3165 acc=95.31%\n",
      "Epoch    5 Iteration [ 321/ 984] Batch loss= 0.2501 acc=97.66%\n",
      "Epoch    5 Iteration [ 331/ 984] Batch loss= 0.2806 acc=96.88%\n",
      "Epoch    5 Iteration [ 341/ 984] Batch loss= 0.3281 acc=95.31%\n",
      "Epoch    5 Iteration [ 351/ 984] Batch loss= 0.2703 acc=98.44%\n",
      "Epoch    5 Iteration [ 361/ 984] Batch loss= 0.3770 acc=93.75%\n",
      "Epoch    5 Iteration [ 371/ 984] Batch loss= 0.1962 acc=99.22%\n",
      "Epoch    5 Iteration [ 381/ 984] Batch loss= 0.3381 acc=94.53%\n",
      "Epoch    5 Iteration [ 391/ 984] Batch loss= 0.3028 acc=93.75%\n",
      "Epoch    5 Iteration [ 401/ 984] Batch loss= 0.3897 acc=92.19%\n",
      "Epoch    5 Iteration [ 411/ 984] Batch loss= 0.2717 acc=97.66%\n",
      "Epoch    5 Iteration [ 421/ 984] Batch loss= 0.2306 acc=96.09%\n",
      "Epoch    5 Iteration [ 431/ 984] Batch loss= 0.3027 acc=95.31%\n",
      "Epoch    5 Iteration [ 441/ 984] Batch loss= 0.2921 acc=96.88%\n",
      "Epoch    5 Iteration [ 451/ 984] Batch loss= 0.3859 acc=95.31%\n",
      "Epoch    5 Iteration [ 461/ 984] Batch loss= 0.3119 acc=96.09%\n",
      "Epoch    5 Iteration [ 471/ 984] Batch loss= 0.3213 acc=96.88%\n",
      "Epoch    5 Iteration [ 481/ 984] Batch loss= 0.2216 acc=97.66%\n",
      "Epoch    5 Iteration [ 491/ 984] Batch loss= 0.3128 acc=97.66%\n",
      "Epoch    5 Iteration [ 501/ 984] Batch loss= 0.3159 acc=93.75%\n",
      "Epoch    5 Iteration [ 511/ 984] Batch loss= 0.3726 acc=93.75%\n",
      "Epoch    5 Iteration [ 521/ 984] Batch loss= 0.2503 acc=96.88%\n",
      "Epoch    5 Iteration [ 531/ 984] Batch loss= 0.2962 acc=95.31%\n",
      "Epoch    5 Iteration [ 541/ 984] Batch loss= 0.3368 acc=96.88%\n",
      "Epoch    5 Iteration [ 551/ 984] Batch loss= 0.3637 acc=93.75%\n",
      "Epoch    5 Iteration [ 561/ 984] Batch loss= 0.2688 acc=96.09%\n",
      "Epoch    5 Iteration [ 571/ 984] Batch loss= 0.2998 acc=94.53%\n",
      "Epoch    5 Iteration [ 581/ 984] Batch loss= 0.2628 acc=96.09%\n",
      "Epoch    5 Iteration [ 591/ 984] Batch loss= 0.3650 acc=90.62%\n",
      "Epoch    5 Iteration [ 601/ 984] Batch loss= 0.3760 acc=92.97%\n",
      "Epoch    5 Iteration [ 611/ 984] Batch loss= 0.3116 acc=93.75%\n",
      "Epoch    5 Iteration [ 621/ 984] Batch loss= 0.2803 acc=97.66%\n",
      "Epoch    5 Iteration [ 631/ 984] Batch loss= 0.4393 acc=91.41%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    5 Iteration [ 641/ 984] Batch loss= 0.3268 acc=94.53%\n",
      "Epoch    5 Iteration [ 651/ 984] Batch loss= 0.2564 acc=97.66%\n",
      "Epoch    5 Iteration [ 661/ 984] Batch loss= 0.2770 acc=95.31%\n",
      "Epoch    5 Iteration [ 671/ 984] Batch loss= 0.2359 acc=98.44%\n",
      "Epoch    5 Iteration [ 681/ 984] Batch loss= 0.3487 acc=93.75%\n",
      "Epoch    5 Iteration [ 691/ 984] Batch loss= 0.2595 acc=96.88%\n",
      "Epoch    5 Iteration [ 701/ 984] Batch loss= 0.3251 acc=93.75%\n",
      "Epoch    5 Iteration [ 711/ 984] Batch loss= 0.3138 acc=96.88%\n",
      "Epoch    5 Iteration [ 721/ 984] Batch loss= 0.3154 acc=95.31%\n",
      "Epoch    5 Iteration [ 731/ 984] Batch loss= 0.3232 acc=94.53%\n",
      "Epoch    5 Iteration [ 741/ 984] Batch loss= 0.2160 acc=99.22%\n",
      "Epoch    5 Iteration [ 751/ 984] Batch loss= 0.2866 acc=96.09%\n",
      "Epoch    5 Iteration [ 761/ 984] Batch loss= 0.2606 acc=97.66%\n",
      "Epoch    5 Iteration [ 771/ 984] Batch loss= 0.2267 acc=96.88%\n",
      "Epoch    5 Iteration [ 781/ 984] Batch loss= 0.2871 acc=96.88%\n",
      "Epoch    5 Iteration [ 791/ 984] Batch loss= 0.3712 acc=91.41%\n",
      "Epoch    5 Iteration [ 801/ 984] Batch loss= 0.3050 acc=96.88%\n",
      "Epoch    5 Iteration [ 811/ 984] Batch loss= 0.2616 acc=97.66%\n",
      "Epoch    5 Iteration [ 821/ 984] Batch loss= 0.2690 acc=96.88%\n",
      "Epoch    5 Iteration [ 831/ 984] Batch loss= 0.2932 acc=96.09%\n",
      "Epoch    5 Iteration [ 841/ 984] Batch loss= 0.2729 acc=97.66%\n",
      "Epoch    5 Iteration [ 851/ 984] Batch loss= 0.3412 acc=94.53%\n",
      "Epoch    5 Iteration [ 861/ 984] Batch loss= 0.2953 acc=96.09%\n",
      "Epoch    5 Iteration [ 871/ 984] Batch loss= 0.4006 acc=90.62%\n",
      "Epoch    5 Iteration [ 881/ 984] Batch loss= 0.3578 acc=94.53%\n",
      "Epoch    5 Iteration [ 891/ 984] Batch loss= 0.4067 acc=90.62%\n",
      "Epoch    5 Iteration [ 901/ 984] Batch loss= 0.2070 acc=97.66%\n",
      "Epoch    5 Iteration [ 911/ 984] Batch loss= 0.2850 acc=96.09%\n",
      "Epoch    5 Iteration [ 921/ 984] Batch loss= 0.3151 acc=94.53%\n",
      "Epoch    5 Iteration [ 931/ 984] Batch loss= 0.3868 acc=91.41%\n",
      "Epoch    5 Iteration [ 941/ 984] Batch loss= 0.3097 acc=94.53%\n",
      "Epoch    5 Iteration [ 951/ 984] Batch loss= 0.2332 acc=96.09%\n",
      "Epoch    5 Iteration [ 961/ 984] Batch loss= 0.2801 acc=96.09%\n",
      "Epoch    5 Iteration [ 971/ 984] Batch loss= 0.2392 acc=97.66%\n",
      "Epoch    5 Iteration [ 981/ 984] Batch loss= 0.3613 acc=93.75%\n",
      "*** 499.5257 Epoch    5 [T] loss= 0.2994 acc=95.69%\t[v] loss= 0.1444 acc=97.15%\t***\n",
      "\n",
      "Epoch    6 Iteration [   1/ 984] Batch loss= 0.3152 acc=96.09%\n",
      "Epoch    6 Iteration [  11/ 984] Batch loss= 0.4016 acc=92.19%\n",
      "Epoch    6 Iteration [  21/ 984] Batch loss= 0.3474 acc=95.31%\n",
      "Epoch    6 Iteration [  31/ 984] Batch loss= 0.2895 acc=96.88%\n",
      "Epoch    6 Iteration [  41/ 984] Batch loss= 0.2849 acc=96.09%\n",
      "Epoch    6 Iteration [  51/ 984] Batch loss= 0.3288 acc=94.53%\n",
      "Epoch    6 Iteration [  61/ 984] Batch loss= 0.2980 acc=95.31%\n",
      "Epoch    6 Iteration [  71/ 984] Batch loss= 0.2550 acc=96.09%\n",
      "Epoch    6 Iteration [  81/ 984] Batch loss= 0.4007 acc=92.19%\n",
      "Epoch    6 Iteration [  91/ 984] Batch loss= 0.3345 acc=95.31%\n",
      "Epoch    6 Iteration [ 101/ 984] Batch loss= 0.2899 acc=96.09%\n",
      "Epoch    6 Iteration [ 111/ 984] Batch loss= 0.3934 acc=93.75%\n",
      "Epoch    6 Iteration [ 121/ 984] Batch loss= 0.3334 acc=93.75%\n",
      "Epoch    6 Iteration [ 131/ 984] Batch loss= 0.2719 acc=98.44%\n",
      "Epoch    6 Iteration [ 141/ 984] Batch loss= 0.2741 acc=96.88%\n",
      "Epoch    6 Iteration [ 151/ 984] Batch loss= 0.3917 acc=92.19%\n",
      "Epoch    6 Iteration [ 161/ 984] Batch loss= 0.3212 acc=95.31%\n",
      "Epoch    6 Iteration [ 171/ 984] Batch loss= 0.2943 acc=95.31%\n",
      "Epoch    6 Iteration [ 181/ 984] Batch loss= 0.2630 acc=96.88%\n",
      "Epoch    6 Iteration [ 191/ 984] Batch loss= 0.3261 acc=95.31%\n",
      "Epoch    6 Iteration [ 201/ 984] Batch loss= 0.2943 acc=98.44%\n",
      "Epoch    6 Iteration [ 211/ 984] Batch loss= 0.2633 acc=96.88%\n",
      "Epoch    6 Iteration [ 221/ 984] Batch loss= 0.3163 acc=96.09%\n",
      "Epoch    6 Iteration [ 231/ 984] Batch loss= 0.2940 acc=96.09%\n",
      "Epoch    6 Iteration [ 241/ 984] Batch loss= 0.3113 acc=95.31%\n",
      "Epoch    6 Iteration [ 251/ 984] Batch loss= 0.3734 acc=92.97%\n",
      "Epoch    6 Iteration [ 261/ 984] Batch loss= 0.2707 acc=95.31%\n",
      "Epoch    6 Iteration [ 271/ 984] Batch loss= 0.2773 acc=95.31%\n",
      "Epoch    6 Iteration [ 281/ 984] Batch loss= 0.3723 acc=93.75%\n",
      "Epoch    6 Iteration [ 291/ 984] Batch loss= 0.3041 acc=96.09%\n",
      "Epoch    6 Iteration [ 301/ 984] Batch loss= 0.2460 acc=98.44%\n",
      "Epoch    6 Iteration [ 311/ 984] Batch loss= 0.3096 acc=94.53%\n",
      "Epoch    6 Iteration [ 321/ 984] Batch loss= 0.2748 acc=96.09%\n",
      "Epoch    6 Iteration [ 331/ 984] Batch loss= 0.3007 acc=96.09%\n",
      "Epoch    6 Iteration [ 341/ 984] Batch loss= 0.2571 acc=96.09%\n",
      "Epoch    6 Iteration [ 351/ 984] Batch loss= 0.3154 acc=95.31%\n",
      "Epoch    6 Iteration [ 361/ 984] Batch loss= 0.3391 acc=94.53%\n",
      "Epoch    6 Iteration [ 371/ 984] Batch loss= 0.3210 acc=92.97%\n",
      "Epoch    6 Iteration [ 381/ 984] Batch loss= 0.2853 acc=96.88%\n",
      "Epoch    6 Iteration [ 391/ 984] Batch loss= 0.3411 acc=94.53%\n",
      "Epoch    6 Iteration [ 401/ 984] Batch loss= 0.2256 acc=96.88%\n",
      "Epoch    6 Iteration [ 411/ 984] Batch loss= 0.2634 acc=97.66%\n",
      "Epoch    6 Iteration [ 421/ 984] Batch loss= 0.2516 acc=94.53%\n",
      "Epoch    6 Iteration [ 431/ 984] Batch loss= 0.3690 acc=91.41%\n",
      "Epoch    6 Iteration [ 441/ 984] Batch loss= 0.3145 acc=96.09%\n",
      "Epoch    6 Iteration [ 451/ 984] Batch loss= 0.3428 acc=93.75%\n",
      "Epoch    6 Iteration [ 461/ 984] Batch loss= 0.2896 acc=96.88%\n",
      "Epoch    6 Iteration [ 471/ 984] Batch loss= 0.3137 acc=95.31%\n",
      "Epoch    6 Iteration [ 481/ 984] Batch loss= 0.3353 acc=94.53%\n",
      "Epoch    6 Iteration [ 491/ 984] Batch loss= 0.3056 acc=95.31%\n",
      "Epoch    6 Iteration [ 501/ 984] Batch loss= 0.2388 acc=96.88%\n",
      "Epoch    6 Iteration [ 511/ 984] Batch loss= 0.3297 acc=94.53%\n",
      "Epoch    6 Iteration [ 521/ 984] Batch loss= 0.2572 acc=95.31%\n",
      "Epoch    6 Iteration [ 531/ 984] Batch loss= 0.3323 acc=95.31%\n",
      "Epoch    6 Iteration [ 541/ 984] Batch loss= 0.3120 acc=97.66%\n",
      "Epoch    6 Iteration [ 551/ 984] Batch loss= 0.2360 acc=96.88%\n",
      "Epoch    6 Iteration [ 561/ 984] Batch loss= 0.3132 acc=94.53%\n",
      "Epoch    6 Iteration [ 571/ 984] Batch loss= 0.3859 acc=90.62%\n",
      "Epoch    6 Iteration [ 581/ 984] Batch loss= 0.2757 acc=99.22%\n",
      "Epoch    6 Iteration [ 591/ 984] Batch loss= 0.3535 acc=92.19%\n",
      "Epoch    6 Iteration [ 601/ 984] Batch loss= 0.4008 acc=92.19%\n",
      "Epoch    6 Iteration [ 611/ 984] Batch loss= 0.3751 acc=94.53%\n",
      "Epoch    6 Iteration [ 621/ 984] Batch loss= 0.3661 acc=95.31%\n",
      "Epoch    6 Iteration [ 631/ 984] Batch loss= 0.2752 acc=96.09%\n",
      "Epoch    6 Iteration [ 641/ 984] Batch loss= 0.3119 acc=93.75%\n",
      "Epoch    6 Iteration [ 651/ 984] Batch loss= 0.3084 acc=96.09%\n",
      "Epoch    6 Iteration [ 661/ 984] Batch loss= 0.2979 acc=97.66%\n",
      "Epoch    6 Iteration [ 671/ 984] Batch loss= 0.3460 acc=93.75%\n",
      "Epoch    6 Iteration [ 681/ 984] Batch loss= 0.3737 acc=91.41%\n",
      "Epoch    6 Iteration [ 691/ 984] Batch loss= 0.3181 acc=95.31%\n",
      "Epoch    6 Iteration [ 701/ 984] Batch loss= 0.3878 acc=93.75%\n",
      "Epoch    6 Iteration [ 711/ 984] Batch loss= 0.2690 acc=93.75%\n",
      "Epoch    6 Iteration [ 721/ 984] Batch loss= 0.3325 acc=93.75%\n",
      "Epoch    6 Iteration [ 731/ 984] Batch loss= 0.3731 acc=92.97%\n",
      "Epoch    6 Iteration [ 741/ 984] Batch loss= 0.3010 acc=95.31%\n",
      "Epoch    6 Iteration [ 751/ 984] Batch loss= 0.2553 acc=97.66%\n",
      "Epoch    6 Iteration [ 761/ 984] Batch loss= 0.2804 acc=95.31%\n",
      "Epoch    6 Iteration [ 771/ 984] Batch loss= 0.3156 acc=95.31%\n",
      "Epoch    6 Iteration [ 781/ 984] Batch loss= 0.2687 acc=97.66%\n",
      "Epoch    6 Iteration [ 791/ 984] Batch loss= 0.3218 acc=95.31%\n",
      "Epoch    6 Iteration [ 801/ 984] Batch loss= 0.3143 acc=96.88%\n",
      "Epoch    6 Iteration [ 811/ 984] Batch loss= 0.3098 acc=92.97%\n",
      "Epoch    6 Iteration [ 821/ 984] Batch loss= 0.3007 acc=96.09%\n",
      "Epoch    6 Iteration [ 831/ 984] Batch loss= 0.2703 acc=97.66%\n",
      "Epoch    6 Iteration [ 841/ 984] Batch loss= 0.2506 acc=96.88%\n",
      "Epoch    6 Iteration [ 851/ 984] Batch loss= 0.2548 acc=96.88%\n",
      "Epoch    6 Iteration [ 861/ 984] Batch loss= 0.3003 acc=95.31%\n",
      "Epoch    6 Iteration [ 871/ 984] Batch loss= 0.3089 acc=96.09%\n",
      "Epoch    6 Iteration [ 881/ 984] Batch loss= 0.2573 acc=98.44%\n",
      "Epoch    6 Iteration [ 891/ 984] Batch loss= 0.3678 acc=92.19%\n",
      "Epoch    6 Iteration [ 901/ 984] Batch loss= 0.3576 acc=92.97%\n",
      "Epoch    6 Iteration [ 911/ 984] Batch loss= 0.3593 acc=92.97%\n",
      "Epoch    6 Iteration [ 921/ 984] Batch loss= 0.3360 acc=93.75%\n",
      "Epoch    6 Iteration [ 931/ 984] Batch loss= 0.3862 acc=91.41%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    6 Iteration [ 941/ 984] Batch loss= 0.3468 acc=92.97%\n",
      "Epoch    6 Iteration [ 951/ 984] Batch loss= 0.3038 acc=96.09%\n",
      "Epoch    6 Iteration [ 961/ 984] Batch loss= 0.2692 acc=94.53%\n",
      "Epoch    6 Iteration [ 971/ 984] Batch loss= 0.2961 acc=92.97%\n",
      "Epoch    6 Iteration [ 981/ 984] Batch loss= 0.3066 acc=95.31%\n",
      "*** 507.0139 Epoch    6 [T] loss= 0.2999 acc=95.71%\t[v] loss= 0.1445 acc=97.14%\t***\n",
      "\n",
      "Epoch    7 Iteration [   1/ 984] Batch loss= 0.2482 acc=97.66%\n",
      "Epoch    7 Iteration [  11/ 984] Batch loss= 0.3311 acc=95.31%\n",
      "Epoch    7 Iteration [  21/ 984] Batch loss= 0.3316 acc=95.31%\n",
      "Epoch    7 Iteration [  31/ 984] Batch loss= 0.4167 acc=91.41%\n",
      "Epoch    7 Iteration [  41/ 984] Batch loss= 0.2650 acc=96.88%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-c79216bfd21c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspaces/Practices/Kaggle Data Analysis/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspaces/Practices/Kaggle Data Analysis/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/Workspaces/Practices/Kaggle Data Analysis/venv/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspaces/Practices/Kaggle Data Analysis/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36m_MulGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m   1224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m   (sx, rx, must_reduce_x), (sy, ry, must_reduce_y) = (\n\u001b[0;32m-> 1226\u001b[0;31m       SmartBroadcastGradientArgs(x, y, grad))\n\u001b[0m\u001b[1;32m   1227\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspaces/Practices/Kaggle Data Analysis/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py\u001b[0m in \u001b[0;36mSmartBroadcastGradientArgs\u001b[0;34m(x, y, grad)\u001b[0m\n\u001b[1;32m     92\u001b[0m       \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m       and isinstance(grad, ops.Tensor)):\n\u001b[0;32m---> 94\u001b[0;31m     \u001b[0msx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m     \u001b[0msy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0mrx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspaces/Practices/Kaggle Data Analysis/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(input, name, out_type)\u001b[0m\n\u001b[1;32m    543\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m   \"\"\"\n\u001b[0;32m--> 545\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Workspaces/Practices/Kaggle Data Analysis/venv/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[0;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m   \"\"\"\n\u001b[0;32m--> 563\u001b[0;31m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Shape\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m     if isinstance(\n\u001b[1;32m    565\u001b[0m         input, (sparse_tensor.SparseTensor, sparse_tensor.SparseTensorValue)):\n",
      "\u001b[0;32m~/Workspaces/Practices/Kaggle Data Analysis/venv/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values)\u001b[0m\n\u001b[1;32m   6170\u001b[0m   \"\"\"\n\u001b[1;32m   6171\u001b[0m   \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6172\u001b[0;31m   \u001b[0min_eager_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6173\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0min_eager_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6174\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minternal_name_scope_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with summary_writer.as_default():\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss = 0.\n",
    "        train_acc = 0.\n",
    "\n",
    "        for iteration in range(start_iteration, iterations):\n",
    "            train_features, train_labels = dataset_train_iterator.get_next()\n",
    "\n",
    "            batch_loss = 0.\n",
    "            batch_acc = 0.\n",
    "\n",
    "            for model in models:\n",
    "                with tf.GradientTape() as tape:\n",
    "                    loss = loss_fn(model, train_features, train_labels, True)\n",
    "                grads = tape.gradient(loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "                batch_loss += loss\n",
    "\n",
    "            batch_loss /= num_models\n",
    "            batch_acc = accuracy_fn(models, train_features, train_labels, False)\n",
    "\n",
    "            train_loss += batch_loss\n",
    "            train_acc += batch_acc\n",
    "\n",
    "            counter += 1\n",
    "\n",
    "            tf.summary.scalar('batch_loss', batch_loss, step=counter)\n",
    "            tf.summary.scalar('batch_acc', batch_acc, step=counter)\n",
    "\n",
    "\n",
    "            if iteration % 10 == 0:\n",
    "                print('Epoch {:4d} Iteration [{:4d}/{:4d}] Batch loss={:7.4f} acc={:.2%}'.format(epoch + 1, iteration + 1, iterations, batch_loss, batch_acc))\n",
    "\n",
    "            if iteration % 50 == 0:\n",
    "                for checkpoint, checkpoint_prefix in zip(checkpoints, checkpoint_prefixes):\n",
    "                    checkpoint.save(file_prefix=checkpoint_prefix + '-{}'.format(counter))\n",
    "\n",
    "        train_loss /= iterations\n",
    "        train_acc /= iterations\n",
    "\n",
    "        val_features, val_labels = dataset_val_iterator.get_next()\n",
    "        val_loss = 0.\n",
    "        for model in models:\n",
    "            val_loss += loss_fn(model, val_features, val_labels, False)\n",
    "        val_loss /= num_models\n",
    "        val_acc = accuracy_fn(models, val_features, val_labels, False)\n",
    "        \n",
    "        tf.summary.scalar('train_loss', train_loss, step=counter)\n",
    "        tf.summary.scalar('train_acc', train_acc, step=counter)\n",
    "        tf.summary.scalar('val_loss', val_loss, step=counter)\n",
    "        tf.summary.scalar('val_acc', val_acc, step=counter)\n",
    "\n",
    "        print('*** {:8.4f} Epoch {:4d} [T] loss={:7.4f} acc={:.2%}\\t[v] loss={:7.4f} acc={:.2%}\\t***\\n'.format(time.time() - start_time, epoch + 1, train_loss, train_acc, val_loss, val_acc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer digit_model is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer digit_model_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer digit_model_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer digit_model_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer digit_model_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "loss =  0.1445 acc = 97.14%\n"
     ]
    }
   ],
   "source": [
    "features, labels = dataset_val_iterator.get_next()\n",
    "loss = 0.\n",
    "for model in models:\n",
    "    loss += loss_fn(model, features, labels, False)\n",
    "loss /= num_models\n",
    "\n",
    "acc = accuracy_fn(models, features, labels, False)\n",
    "\n",
    "print('loss = {:7.4f} acc = {:.2%}'.format(loss, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = np.zeros((x_test.shape[0], 10))\n",
    "for model in models:\n",
    "    results += model(x_test).numpy()\n",
    "labels = np.argmax(results, axis=-1)\n",
    "\n",
    "submission = submission = pd.concat([pd.Series(range(1, x_test.shape[0] + 1)), pd.Series(labels)], axis=1)\n",
    "submission.columns = ['ImageId', 'Label']\n",
    "submission.to_csv('./data/submission/submission.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
